{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Active Sciencing\" with Reusable Workflows\n",
    "\n",
    "By Kyle Cranmer & Lukas Heinrich June 4, 2016\n",
    "\n",
    "Based on earlier work with NYU CDS masters students Manoj Kumar, Phil Yeres, and Michele Ceru and discussions with Brenden Lake and Gilles Louppe.\n",
    "\n",
    "Define:\n",
    "\n",
    " 1) $\\phi$ : Experimental configuration\n",
    "\n",
    " 2) $\\theta$: Parameters that we would like to infer from the experimental data\n",
    "\n",
    " 3) $X$ : Data generated from the experiment or simulator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "import emcee\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Perform Experiment, Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulator(theta, phi, n_samples=100):\n",
    "    return np.random.normal(loc=theta, scale=2 + np.sin(phi), size=n_samples)\n",
    "\n",
    "def collect_data(phi, n_samples=100):\n",
    "    # 3.141 is the unknown parameter we are trying to measure\n",
    "    # best experimental setting is 3pi/2 (or any multiple of it)\n",
    "    return simulator(3.141, phi, n_samples)\n",
    "\n",
    "phi = 1.5 * np.pi\n",
    "data = collect_data(phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Bayesian Prior â†’ Posterior Update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnprior(theta, prior):\n",
    "    # XXX incorporate the prior passed in here\n",
    "    if 0. < theta < 2*np.pi:\n",
    "        return 0.\n",
    "    return -np.inf\n",
    "\n",
    "def lnlike(theta, y, phi):\n",
    "    mean, std = theta, 2 + np.sin(phi)\n",
    "    return np.log(norm(mean, std).pdf(y).prod())\n",
    "\n",
    "def lnprob(theta, x, prior, phi):\n",
    "    lp = lnprior(theta, prior)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + lnlike(theta, x, phi)\n",
    "\n",
    "def calculate_posterior(prior, data, phi):\n",
    "    \"\"\"Compute samples from the posterior\"\"\"\n",
    "    ndim, n_walkers = 1, 10\n",
    "    pos = [2. + 1e-1*np.random.randn(ndim) for i in range(n_walkers)]\n",
    "    \n",
    "    sampler = emcee.EnsembleSampler(n_walkers, 1, lnprob, args=(data, prior, phi))\n",
    "    pos, prob, state = sampler.run_mcmc(pos, 100)\n",
    "    \n",
    "    sampler.reset()\n",
    "    pos, prob, state = sampler.run_mcmc(pos, 300)\n",
    "    \n",
    "    return Distribution(prior.name, prior.range, sampler.flatchain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = Distribution('mean', range=(0, 2*np.pi))\n",
    "posterior = calculate_posterior(prior, data, phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCdVlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10aca9410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "posterior.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Optimize Experimental Configuration\n",
    "\n",
    "Based on the updated posterior $p(\\theta)$ we will consider future experiments with configuration $\\phi$. For each of those configurations, we will run several simulations of the experiment and perform inference on those simulated datasets to estimate the expected information gain (EIG)\n",
    "\n",
    "\\begin{equation}\n",
    "EIG(\\phi) =  \\int dx d\\theta \\; p(x | \\theta) p(\\theta) \\big [ H\\left [P(\\theta) \\right] - H\\left[ P(\\theta\\, |\\, x) \\right] \\big ] \\approx \\int dx  \\; p(x | \\theta_{MAP}) \\big [ H\\left [P(\\theta) \\right] - H\\left[ P(\\theta\\, |\\, x) \\right] \\big ]\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "H\\left [P(\\theta) \\right] = \\int P(\\theta) \\log P(\\theta) d\\theta \n",
    "\\end{equation}\n",
    "\n",
    "To efficiently optimize $EIG[\\phi]$ we will use an active learning procedure like Bayesian Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def info_gain(p1, p2):\n",
    "    return p1.entropy() - p2.entropy()\n",
    "\n",
    "def expected_information_gain(phi, prior):\n",
    "    'calculate the expression above using workflow for simulations'\n",
    "    n_simulations = 2\n",
    "    \n",
    "    #need to pass in prior through some extra arguments\n",
    "    \n",
    "    # use saddle-point approximation\n",
    "    theta_map = prior.map()\n",
    "    \n",
    "    eig = np.zeros(n_simulations)\n",
    "    \n",
    "    for i_sim in range(n_simulations):\n",
    "        # external workflow provides simulated data\n",
    "        sim_data = simulator(theta_map, phi)\n",
    "\n",
    "        #external workflow uses simulator to provide likelihood \n",
    "        sim_posterior = calculate_posterior(prior, sim_data, phi)\n",
    "        eig[i_sim] = info_gain(prior, sim_posterior)\n",
    "        \n",
    "    #check for outliers?\n",
    "    \n",
    "    return np.mean(eig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initial phi ~ 1.456 (or so)\n",
    "phis = np.linspace(0., 2*np.pi, 20)\n",
    "eigs = [expected_information_gain(p, posterior) for p in phis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(phis, eigs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initial phi = 3.56\n",
    "phis = np.linspace(0., 2*np.pi, 20)\n",
    "eigs = [expected_information_gain(p, posterior) for p in phis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(phis, eigs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initial phi = 1.5*np.pi\n",
    "phis = np.linspace(0., 2*np.pi, 20)\n",
    "eigs = [expected_information_gain(p, posterior) for p in phis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(phis, eigs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use scikit-optimize to optimize phi\n",
    "from skopt import gp_minimize\n",
    "from skopt.plots import plot_evaluations\n",
    "\n",
    "def design_next_experiment(prior):\n",
    "    bounds = [(-5.0, 10.0), (0.0, 15.0)]\n",
    "    n_calls = 50\n",
    "\n",
    "    opt_result = gp_minimize(expected_information_gain, bounds, n_calls=n_calls, random_state=4)\n",
    "\n",
    "    _ = plot_evaluations(opt_result, bins=10)\n",
    "    return opt_phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phi = 0.\n",
    "prior_theta = Distribution() #simple class for range, density for emcee via KDE or histogram, MAP?\n",
    "\n",
    "n_science_iterations = 10\n",
    "\n",
    "phi_history = []\n",
    "prior_history = []\n",
    "\n",
    "for i_experiment in range(science_iterations):\n",
    "    phi_history.append(phi)\n",
    "    prior_history.append(prior)\n",
    "\n",
    "    # run experiment with configuration given by phi\n",
    "    real_data = collect_data(phi)\n",
    "\n",
    "    #update new prior = posterior from previous experiment\n",
    "    prior = calculate_posterior(prior, real_data, phi)\n",
    "    \n",
    "    #design new experiment given current knowledge\n",
    "    phi=design_next_experiment(prior)\n",
    "    \n",
    "\n",
    "#make some plots of prior, and phi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#put this in a separate file\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "class Distribution:\n",
    "    '''\n",
    "    member vars\n",
    "        variable names\n",
    "        ranges  (for use with george)\n",
    "    methods\n",
    "        density estimate (KDE or histogram) (for use with emcee)\n",
    "        MAP (for saddle point approximation)\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, name, range, samples=None):\n",
    "        self.range = range\n",
    "        self.name = name\n",
    "        if samples is None:\n",
    "            self.samples = np.random.uniform(range[0], range[1], size=10000)\n",
    "        else:\n",
    "            self.samples = samples\n",
    "        \n",
    "    def map(self):\n",
    "        prob, edges = np.histogram(self.samples, range=self.range, bins=100)\n",
    "        prob = prob.clip(min=0.0000000001)\n",
    "        return edges[np.argmax(prob)]\n",
    "    \n",
    "    def entropy(self):\n",
    "        prob, edges = np.histogram(self.samples, range=self.range, bins=100)\n",
    "        prob = prob.clip(min=0.0000000001)\n",
    "        return entropy(prob)\n",
    "    \n",
    "    def hist(self):\n",
    "        plt.hist(self.samples, range=self.range, bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
